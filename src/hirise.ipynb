{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiRISENet-Tiny\n",
    "\n",
    "A tiny neural network classifier for Mars HiRISE images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mars HiRISE (High Resolution Imaging Science Experiment) [[1]](#1) is a camera on board the Mars Reconnaissance\n",
    "Orbiter which has been orbiting and studying Mars since 2006. A product of this payload's years of service has been the\n",
    "curation of the Mars orbital image (HiRISE) labeled data set [[2]](#2) by NASA. This dataset provides a curated set of\n",
    "labelled images of the Martian terrain from an orbital perspective. **Figure 1** shows one of the first images taken\n",
    "with the HiRISE camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--- Public Domain, https://commons.wikimedia.org/w/index.php?curid=656514 >\n",
    "--->\n",
    "![First HiRISE Image](../assets/images/mro_first_image.jpg \"Figure 1. The first orbital image captured by HiRISE\")\n",
    "\n",
    "**Figure 1. Crop of one of the first images of Mars from the HiRISE camera.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2018, Wagstaff et. al. [[3]](#3) set out to train a deep learning model to enable scientists & researchers to conduct\n",
    "advanced queries of the HiRISE data in NASA's planetary imagery database. With the first edition of the dataset, the\n",
    "authors fine-tuned the AlexNet convolutional neural network on the data. This initial dataset contained 3,820 \n",
    "greyscale images and consisted of the labels crater, dark dune, bright dune, dark slope streak, other and edge. With\n",
    "this dataset, the model achieved 99.1%, 88.1%, and 90.6% accuracy across their training, validation, and test sets\n",
    "respectively.\n",
    "\n",
    "Following this effort, Wagstaff et. al. published a follow up paper [[4]](#4) which expanded the initial dataset to\n",
    "a version 3.2. This expanded dataset consists of a total of 64,947 landmark images. These images have been preprocessed\n",
    "and cropped to a 227x227 size similar to the first edition. In v3.2, a subset of these images, 9,022, were augmented \n",
    "using a variety of image augmentation techniques applied individually to the subset of images. This artificially\n",
    "expanded the available data. The authors also introduced the classes impact ejecta, spiders, and swiss cheese while\n",
    "removing edge.\n",
    "\n",
    "![HiRISE v3.2 Class Imagse](../assets/images/hirise_v3_classes.png)\n",
    "\n",
    "The authors also fine-tuned the AlexNet model on the new dataset. For this iteration, the authors analyzed the model's\n",
    "confidence and utilized calibration techniques to make the model more reliable. With this approach and improved dataset,\n",
    "the authors managed to improve the model's classification accuracy to\n",
    "{\"train\": 99.6%, \"val\": 88.6%, \"test\": 92.8%}.\n",
    "\n",
    "![Dataset Class Distribution](../assets/images/hirise_dataset_class_distribution.png)\n",
    "\n",
    "An important thing to note is that the class distribution is pretty unbalanced. Images of \"Other\" significantly\n",
    "dominate the dataset while \"Impact Ejecta\" constitutes a small portion of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this Project\n",
    "\n",
    "In this project I challenged myself to develop a pipeline for training and\n",
    "deployment of a neural network to run on the NVIDIA Jetson Nano 2GB. This neural\n",
    "network shall be able to classify the greyscale HiRISE images.\n",
    "\n",
    "My goal is to train a modern convolutional neural network that is more resource\n",
    "efficient than AlexNet and, hopefully, as good as the author's trained AlexNet.\n",
    "\n",
    "As a flight software engineer, I am focused on creating a trusted model that is\n",
    "also resource efficient. As part of this project I also want to analyze and\n",
    "evaluate the resource utilization and performance of the model.\n",
    "\n",
    "Previous work by Dunkel et. al. [[5]](#5) provides some clues as to metrics\n",
    "machine learning practitioners should be cognisant of for a space-based\n",
    "environment. Based on the information the authors provided, I have derived some\n",
    "requirements that outline metrics I will be collecting.\n",
    "\n",
    "Key metrics I would like to collect include:\n",
    "\n",
    "* Inference time of the model on a single input image\n",
    "* Peak RAM utilization of the model\n",
    "* Disk utilization of the model\n",
    "* Energy consumption of the model\n",
    "\n",
    "#### Requirements\n",
    "\n",
    "| Requirement | Description                                                                                    | Description                                           | Verification Method |\n",
    "|-------------|------------------------------------------------------------------------------------------------|-------------------------------------------------------|---------------------|\n",
    "| HIRISE-001  | A neural network model shall be trained on the HiRISE v3.2 dataset.                            | The goal of the project is to classify HiRISE images. | Test Set Evaluation |\n",
    "| HIRISE-002  | A neural network model shall achieve a minimum of 80% accuracy on the test dataset.            | The model needs to be nearly as good as AlexNet.      | Test Set Evaluation |\n",
    "| HIRISE-003  | A neural network model shall execute on the NVIDIA Jetson Nano 2GB.                            | The flight software board is the NVIDIA Jetson Nano.  | Inspection          |\n",
    "| HIRISE-004  | A neural network model inference time shall not exceed 1,069 ms on the NVIDIA Jetson Nano CPU. | The existing benchmark for HiRISENet is 1,069 ms.     | Profiling Test      |\n",
    "| HIRISE-005  | A neural network model inference time shall not exceed 234 ms on the NVIDIA Jetson Nano GPU.   | The existing benchmark for HiRISENet is 234 ms.       | Profiling Test      |\n",
    "| HIRISE-006  | A neural network energy consumption shall not exceed 10.7 J on the NVIDIA Jetson Nano CPU.     | The existing benchmark for HiRISENet is 10.7 J.       | Profiling Test      |\n",
    "| HIRISE-007  | A neural network energy consumption shall not exceed 2.3 J on the NVIDIA Jetson Nano CPU.      | The existing benchmark for HiRISENet is 2.3 J.        | Profiling Test      |\n",
    "| HIRISE-008  | A neural network model parameter size shall not exceed 233 MB.                                 | AlexNet in PyTorch 2.5 is ~233 MB in size.            | Profiling Test      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Lets get some basic project infrastructure set up first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__CUDA VERSION: 90100\n",
      "__Number CUDA Devices: 1\n",
      "__CUDA Device Name: NVIDIA RTX A2000 8GB Laptop GPU\n",
      "__CUDA Device Total Memory [GB]: 8.58947584\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.gpu_management import GPUManager\n",
    "\n",
    "from data.dataset import HiRISE\n",
    "from data.split_type import SplitType\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from tempfile import TemporaryDirectory\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# ? How does this work?\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "\n",
    "device = GPUManager.enable_gpu_if_available()\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    GPUManager.cleanup_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = TemporaryDirectory()\n",
    "temp_dir.name\n",
    "temp_dir.name = \"/tmp/tmpr2oojxfd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "The first thing we should do is gain a deep understanding of our dataset. As\n",
    "mentioned in the introduction, the model we train is a direct reflection of our\n",
    "data. That is, the model _is_ the data.\n",
    "\n",
    "The HiRISE v3.2 [[2]](#2) dataset has the following characteristics:\n",
    "\n",
    "* **Number of Classes:** The dataset comprises of 8 different classes, each representing different Martian terrain features: bright dune, crater, dark dune, impact ejecta, other, slope streak, spider, swiss cheese.\n",
    "\n",
    "* **Images:** The dataset contains 64,947 227x227 images. The images have been pre-processed and a subset of the original images were augmented with the following transformations: 90° clockwise rotation, 180° clockwise rotation, 270° clockwise rotation, horizontal flip, vertical flip, random brightness adjustment.\n",
    "\n",
    "* **File Format:** The images are stored in JPG format.\n",
    "\n",
    "* **Directory Structure:** The data has been split by the authors into a training, validation, and test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can feed our dataset into a model, we need to transform it into a\n",
    "format that the model can understand. For this, we use the `transforms` module\n",
    "from `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": v2.Compose(\n",
    "        [\n",
    "            v2.Resize((227, 227)),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            # v2.Normalize(mean=[0.5], std=[0.5], inplace=True)\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": torchvision.transforms.v2.Compose(\n",
    "        [\n",
    "            v2.Resize((227, 227)),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            # v2.Normalize(mean=[0.5], std=[0.5], inplace=True)\n",
    "        ]\n",
    "    ),\n",
    "    \"test\": torchvision.transforms.v2.Compose(\n",
    "        [v2.Resize((227, 227)), v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the transforms we use are pretty standard for image data.\n",
    "We first resize each image to 227x227. The images should already be this size\n",
    "but you can never be too careful. The `v2.ToImage()` and `v2.ToDtype(...)`\n",
    "are the new PyTorch v2 recommended way of transforming an image to a tensor.\n",
    "The old `ToTensor()` is deprecates as of this writing.\n",
    "\n",
    "Lastly, the `v2.Normalize()` transformation normalizes the tensor image. The\n",
    "values are simply the midpoint of [0.0, 1.0].\n",
    "Normalization helps improve the convergence speed and overall performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected argument value expression (3108753032.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    root_dir=,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected argument value expression\n"
     ]
    }
   ],
   "source": [
    "data_folder: Path = Path(\"/tmp/.hirise\")\n",
    "\n",
    "train_dataset = HiRISE(\n",
    "    root_dir=data_folder,\n",
    "    split_type=SplitType.TRAIN,\n",
    "    transform=data_transforms[\"train\"],\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "val_dataset = HiRISE(\n",
    "    root_dir=data_folder,\n",
    "    split_type=SplitType.VAL,\n",
    "    transform=data_transforms[\"val\"],\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_dataset = HiRISE(\n",
    "    root_dir=data_folder,\n",
    "    split_type=SplitType.TEST,\n",
    "    transform=data_transforms[\"test\"],\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our datasets, lets see what these images actually look like.\n",
    "Lets plot one of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.show_image_per_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the images we show, it's clear that some of these terrains are very\n",
    "similar to each other. Two particular classes that stand out to me are the\n",
    "crater and impact ejecta. They both look like craters with the distinguishing\n",
    "feature being the brighter material surrounding the impact crater. \n",
    "\n",
    "It also seems like a crater in this dataset is an impact zone that is larger\n",
    "than an impact ejecta image. So then, what if the impact ejecta has been covered\n",
    "or eroded in the image? I wonder if the model will have some difficulties with\n",
    "that.\n",
    "\n",
    "As mentioned by the authors of the dataset, the \"other\" class is a catch-all\n",
    "class to capture anything that doesn't quite fit into the other classes. They\n",
    "also mention that this \"other\" class makes up the overwhelming majority of the\n",
    "data.\n",
    "\n",
    "Lets take a look at that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.show_class_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the distribution plot we can see that the \"Other\" landmarks significantly\n",
    "dominate the class distribution. This clear imbalance of data will need to be\n",
    "accounted for in our model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Model\n",
    "\n",
    "To train our model we first need to create `DataLoader` for each of our data\n",
    "splits. The `DataLoader` is a PyTorch class that facilitates iterating on our\n",
    "data as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    name: torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=32, shuffle=True, num_workers=8, pin_memory=True\n",
    "    )\n",
    "    for name, dataset in zip(\n",
    "        [\"train\", \"val\", \"test\"], [train_dataset, val_dataset, test_dataset]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of HiRISENet used a pre-trained AlexNet model and fine-tuned it to\n",
    "the HiRISE dataset. They achieved incredible results with that model and employed\n",
    "techniques like calibration to guarantee a fair and balanced model.\n",
    "\n",
    "In this project I was first enticed by the idea of using a pre-trained vision \n",
    "transformer. However, given the limited size of the dataset I opted for Liu et al. \n",
    "\"ConvNeXt\" [[6]](#6) architecture. Convolutional neural networks are a more\n",
    "mature and well understood architecture. As a flight software engineer I like\n",
    "ol' reliable tried and true over flashy state-of-the-art. Additionally, the\n",
    "ConvNeXt architecture performs just as well as vision transformers on some\n",
    "tasks \"while maintaining the simplicity and efficiency of standard ConvNets.\"\n",
    "\n",
    "For the ConvNext architecture, the authors opted for layer normalization instead\n",
    "of batch normalization. The former being a bit more computationally efficient.\n",
    "Additionally, the authors implemented depthwise separable convolutions for\n",
    "even more computational efficiency. Other parts of this architecture \"borrowed\"\n",
    "or were inspired by mechanisms in vision transformers like self-attention.\n",
    "\n",
    "For this project, we will use the \"tiny\" version of the ConvNeXt model which is\n",
    "pre-trained on ImageNet data. We will then fine-tune the model i.e. transfer\n",
    "learning to get it to learn our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.convnext_tiny(\n",
    "    weights=torchvision.models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The existing ConvNeXt model expects 3 input channels while the HiRISE image\n",
    "data is single channel. This means we have to modify the input layer of the model\n",
    "to accomodate for this discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only change the input channel and keep everything else the same\n",
    "model.features[0][0] = torch.nn.Conv2d(\n",
    "    1, 96, kernel_size=(4, 4), stride=(4, 4), device=device\n",
    ")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I simply utilized a pre-trained ConvNeXt model that's available\n",
    "from PyTorch utilizing the the smaller IMAGENET 1K weights. This makes the model\n",
    "size ~109 MB which fullfills our HIRISE-008 requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import model_insights\n",
    "\n",
    "model_insights.calc_model_size(model, show=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model Evaluation\n",
    "\n",
    "To follow some machine learning best practices, we should first establish a\n",
    "baseline. That means no crazy augmentations and even a stupid simple model. The\n",
    "objective with the initial training round is to just make sure everything is\n",
    "set up correctly and that our stupid model can train on the data. For this\n",
    "iteration we want to reduce the number of knobs and variables to the absolute\n",
    "minimum.\n",
    "\n",
    "Since the authors of the HiRISE dataset have done a lot of the the heavy\n",
    "lifting for us, we will jump right into training our model. For this round we\n",
    "will skip any regularization techniques and just train the model directly on\n",
    "the available data. We will modify our setup like adding augmentations based on\n",
    "the initial results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE: float = 1e-3\n",
    "BATCH_SIZE: int = 32\n",
    "EPOCHS: int = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss function we will use `CrossEntropyLoss`. This is a fairly standard\n",
    "loss function and is commonly used in machine learning. Similarly, the `Adam`\n",
    "optimizer is standard and pretty foolproof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "# ? Do we need a scheduler?\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {\n",
    "    name: len(dataset)\n",
    "    for name, dataset in zip(\n",
    "        [\"train\", \"val\", \"test\"], [train_dataset, val_dataset, test_dataset]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    return running_loss, running_corrects\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    return running_loss, running_corrects\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=3):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "        history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Each epoch has a training and validation phase\n",
    "            train_loss, train_corrects = train_epoch(\n",
    "                model, dataloaders[\"train\"], criterion, optimizer, device\n",
    "            )\n",
    "            # scheduler.step()\n",
    "            val_loss, val_corrects = validate_epoch(\n",
    "                model, dataloaders[\"val\"], criterion, device\n",
    "            )\n",
    "\n",
    "            train_loss /= dataset_sizes[\"train\"]\n",
    "            train_acc = train_corrects.double() / dataset_sizes[\"train\"]\n",
    "            val_loss /= dataset_sizes[\"val\"]\n",
    "            val_acc = val_corrects.double() / dataset_sizes[\"val\"]\n",
    "\n",
    "            history.append([train_acc, val_acc, train_loss, val_loss])\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{num_epochs - 1}: \"\n",
    "                f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "            # deep copy the model\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(\n",
    "            f\"\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\"\n",
    "        )\n",
    "        print(f\"Best val Acc: {best_acc:.4f}\")\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, model_ft_history = train_model(\n",
    "    model, criterion, optimizer, None, num_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\">[1]</a> \n",
    "Wikipedia contributors. (2024, November 19). HiRISE. Wikipedia. https://en.wikipedia.org/wiki/HiRISE\n",
    "\n",
    "<a id=\"2\">[2]</a> \n",
    "Gary Doran, Emily Dunkel, Steven Lu, & Kiri Wagstaff. (2020). Mars orbital image (HiRISE) labeled data set version 3.2 (3.2.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.4002935\n",
    "\n",
    "<a id=\"3\">[3]</a>\n",
    "Wagstaff, K., Lu, Y., Stanboli, A., Grimes, K., Gowda, T., & Padams, J. (2018). Deep Mars: CNN Classification of Mars Imagery for the PDS Imaging Atlas. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). https://doi.org/10.1609/aaai.v32i1.11404\n",
    "\n",
    "<a id=\"4\">[4]</a>\n",
    "Wagstaff, Kiri, et al. Mars Image Content Classification: Three Years of NASA Deployment and Recent Advances. arXiv:2102.05011, arXiv, 9 Feb. 2021. arXiv.org, https://doi.org/10.48550/arXiv.2102.05011.\n",
    "\n",
    "<a id=\"5\">[5]</a>\n",
    "Dunkel, Emily R., et al. “Benchmarking Deep Learning Models on Myriad and Snapdragon Processors for Space Applications.” Journal of Aerospace Information Systems, vol. 20, no. 10, Oct. 2023, pp. 660–74. DOI.org (Crossref), https://doi.org/10.2514/1.I011216.\n",
    "\n",
    "<a id=\"6\">[6]</a>\n",
    "Liu, Zhuang, et al. A ConvNet for the 2020s. arXiv:2201.03545, arXiv, 2 Mar. 2022. arXiv.org, https://doi.org/10.48550/arXiv.2201.03545.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hirisenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
